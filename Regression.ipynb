{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28322c84-d4bf-4e69-b04b-deefd8b30a72",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786d58a-398e-481d-8c8e-89c1c034b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS -R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. In other words, it indicates how well the independent variables explain the variability of the dependent variable.\n",
    "\n",
    "The R-squared value ranges from 0 to 1, where:\n",
    "\n",
    "0 indicates that the model does not explain any of the variability in the dependent variable.\n",
    "1 indicates that the model explains all the variability in the dependent variable.\n",
    "The R-squared value is calculated using the following formula:\n",
    "\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "SSR (Sum of Squared Residuals) is the sum of the squared differences between the actual values of the dependent variable and the predicted values by the regression model.\n",
    "SST (Total Sum of Squares) is the sum of the squared differences between the actual values of the dependent variable and the mean of the dependent variable.\n",
    "In simpler terms, R-squared measures the proportion of the total variability in the dependent variable that is captured by the model. A higher R-squared value generally indicates a better fit of the model to the data, suggesting that a larger portion of the variability in the dependent variable is explained by the independent variables.\n",
    "\n",
    "However, it's important to note that R-squared should not be the sole criterion for evaluating the goodness of a regression model. It doesn't tell us about the correctness of the model specifications or whether the chosen independent variables are appropriate. Additionally, a high R-squared value does not imply causation; it only indicates the strength of the relationship between the variables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23463043-bb6c-4f1d-8c1f-1490ab581ffa",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd2618-6747-45c4-840e-bc2e29448e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent variables in the regression model. While the regular R-squared measures the proportion of the variance in the dependent variable explained by the independent variables, adjusted R-squared adjusts this value to penalize for the inclusion of unnecessary variables that do not contribute significantly to the model.\n",
    "\n",
    "The formula for adjusted R-squared is given by:\n",
    "\n",
    "\n",
    "  is the regular R-squared value.\n",
    "�\n",
    "n is the number of observations in the sample.\n",
    "�\n",
    "k is the number of independent variables in the model.\n",
    "The adjusted R-squared penalizes models with more independent variables, adjusting the value based on the number of variables included. This adjustment is important because the regular R-squared tends to increase as more variables are added to the model, even if those variables do not actually improve the model's explanatory power. The adjusted R-squared accounts for this and provides a more accurate assessment of the model's goodness of fit.\n",
    "\n",
    "In summary, while the regular R-squared provides a measure of the proportion of variance explained, the adjusted R-squared offers a more conservative evaluation by considering the number of variables in the model. It helps to guard against overfitting and provides a better indication of the model's effectiveness in explaining the variance in the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87e03b-e42f-4496-9273-e625392fe210",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95948c-5a3c-436e-8b1c-dff755e6e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing models with a different number of independent variables or when assessing the goodness of fit of a regression model that includes multiple predictors. Here are some situations where adjusted R-squared is particularly useful:\n",
    "\n",
    "Comparing Models with Different Numbers of Variables:\n",
    "Adjusted R-squared is especially helpful when you are comparing models that have a different number of independent variables. Regular R-squared tends to increase as more variables are added, even if those variables do not contribute significantly to explaining the variation in the dependent variable. Adjusted R-squared penalizes the inclusion of unnecessary variables, providing a fairer comparison between models.\n",
    "\n",
    "Guarding Against Overfitting:\n",
    "Overfitting occurs when a model is too complex, capturing noise in the data rather than the underlying patterns. Adjusted R-squared helps guard against overfitting by penalizing models that include too many variables, encouraging the selection of a more parsimonious model that is likely to generalize better to new data.\n",
    "\n",
    "Model Selection:\n",
    "When you are building regression models and selecting variables to include, adjusted R-squared can be a useful criterion for assessing the goodness of fit. It helps in choosing a model that strikes a balance between explaining the variation in the dependent variable and avoiding the inclusion of irrelevant variables.\n",
    "\n",
    "Multiple Regression Analysis:\n",
    "In multiple regression, where there are several independent variables, adjusted R-squared is often preferred over regular R-squared for assessing the overall fit of the model. This is because the regular R-squared may increase simply by adding more predictors, even if they don't improve the model's explanatory power.\n",
    "\n",
    "Sample Size Variation:\n",
    "Adjusted R-squared is less sensitive to changes in sample size compared to regular R-squared. When the number of observations in the sample changes, regular R-squared may give a misleading impression of model performance, whereas adjusted R-squared adjusts for this by considering the number of predictors.\n",
    "\n",
    "In summary, adjusted R-squared is more appropriate when dealing with models with different numbers of variables, helping to make a fair comparison and providing a more accurate assessment of a model's goodness of fit, particularly in the context of multiple regression analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4275629f-c559-478a-81ab-7df1b2ea5afc",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metricscalculated, and what do they represent?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d26cf-9953-4263-add2-4f4b939297ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Root Mean Squared Error (RMSE), Mean Squared Error (MSE), and Mean Absolute Error (MAE) are commonly used metrics in regression analysis to evaluate the performance of predictive models. These metrics quantify the differences between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE is calculated as the average of the squared differences between the predicted and actual values. The formula is given by:\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "where \n",
    "�\n",
    "n is the number of observations, \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the actual value, and \n",
    "�\n",
    "^\n",
    "�\n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    "  is the predicted value. Squaring the differences emphasizes larger errors.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "RMSE is the square root of the MSE and provides a measure of the average magnitude of the errors in the same unit as the dependent variable. The formula is:\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "�\n",
    "RMSE= \n",
    "MSE\n",
    "​\n",
    " \n",
    "RMSE is more sensitive to large errors than MSE because of the square root operation.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE is calculated as the average of the absolute differences between the predicted and actual values. The formula is:\n",
    "\n",
    "MAE is less sensitive to outliers compared to MSE and RMSE because it doesn't involve squaring the differences.\n",
    "\n",
    "These metrics serve the following purposes:\n",
    "\n",
    "MSE and RMSE: Emphasize larger errors and are particularly sensitive to outliers. RMSE is commonly used when the errors follow a normal distribution, and it penalizes larger errors more heavily.\n",
    "\n",
    "MAE: Provides an average of the absolute errors and is less sensitive to outliers. It is often used when the distribution of errors is not assumed to be normal, and the focus is on the magnitude of errors rather than their squared values.\n",
    "\n",
    "When interpreting these metrics, lower values indicate better model performance. It's essential to choose the metric that aligns with the specific goals and characteristics of the problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8056c64-f15d-4701-aded-25651b4ca640",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74eb9b-02d0-40fe-96af-830a4ed8771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages:\n",
    "1. Mean Squared Error (MSE) and Root Mean Squared Error (RMSE):\n",
    "Advantages:\n",
    "\n",
    "Emphasis on Larger Errors: MSE and RMSE give higher weights to larger errors due to the squaring operation. This is beneficial when large errors are considered more critical.\n",
    "Mathematical Properties: The squaring operation makes MSE and RMSE amenable to mathematical analysis and optimization.\n",
    "Disadvantages:\n",
    "\n",
    "Sensitivity to Outliers: MSE and RMSE can be sensitive to outliers since they amplify the impact of larger errors. Outliers can disproportionately influence the results.\n",
    "2. Mean Absolute Error (MAE):\n",
    "Advantages:\n",
    "\n",
    "Robustness to Outliers: MAE is less sensitive to outliers because it does not involve squaring the errors. It provides a more balanced view of the overall model performance.\n",
    "Interpretability: The metric is easy to interpret since it represents the average absolute difference between predicted and actual values.\n",
    "Disadvantages:\n",
    "\n",
    "Equal Weight to All Errors: MAE treats all errors equally, regardless of their magnitude. In some cases, this might not align with the desired emphasis on larger errors.\n",
    "Mathematical Properties: The absolute value operation in MAE makes it less amenable to mathematical analysis compared to MSE and RMSE.\n",
    "Considerations for All Metrics:\n",
    "1. Scale of the Metric:\n",
    "Advantages:\n",
    "Interpretability: All three metrics (MSE, RMSE, and MAE) have straightforward interpretations in terms of the scale of the dependent variable, making them easy to communicate to non-technical stakeholders.\n",
    "2. Choice of Metric:\n",
    "Advantages:\n",
    "Flexibility: The choice of metric depends on the specific goals and characteristics of the problem. Researchers can select the metric that aligns with the relative importance of different types of errors in their context.\n",
    "Conclusion:\n",
    "The selection of evaluation metrics in regression analysis involves trade-offs based on the nature of the problem, the desired properties of the metric, and the assumptions about the distribution of errors. Practitioners often consider the specific goals of their analysis, the characteristics of the data, and the interpretability of the chosen metric when making decisions about model evaluation. It's also common to use a combination of metrics to gain a comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161fdc2-68d8-4267-a6d3-bf9046e39785",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d425f-0748-4587-af9a-68a8863eae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to prevent overfitting and improve the model's generalization performance. It introduces a penalty term to the linear regression objective function that discourages the inclusion of unnecessary features by adding the absolute values of the regression coefficients to the optimization problem.\n",
    "\n",
    "The Lasso regularization term is added to the standard linear regression objective function, and the modified objective function to minimize becomes:\n",
    "\n",
    "Objective function with Lasso regularization\n",
    "=\n",
    "Least Squares Loss\n",
    "+\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "Objective function with Lasso regularization=Least Squares Loss+λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "\n",
    "where:\n",
    "\n",
    "Least Squares Loss\n",
    "Least Squares Loss is the standard linear regression loss term.\n",
    "�\n",
    "λ is the regularization parameter (also known as the tuning parameter or penalty strength).\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣ represents the sum of the absolute values of the regression coefficients.\n",
    "Lasso regularization encourages sparsity in the model, meaning it tends to drive some regression coefficients to exactly zero. This property makes Lasso useful for feature selection, as it can effectively eliminate irrelevant or redundant predictors.\n",
    "\n",
    "Differences between Lasso and Ridge regularization:\n",
    "\n",
    "Regularization Term:\n",
    "\n",
    "Lasso: Uses the sum of the absolute values of the coefficients (\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣).\n",
    "Ridge: Uses the sum of the squared values of the coefficients (\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " ).\n",
    "Effect on Coefficients:\n",
    "\n",
    "Lasso: Tends to produce sparse solutions by driving some coefficients to exactly zero, effectively selecting a subset of the most relevant features.\n",
    "Ridge: Penalizes large coefficients but rarely drives them to zero, leading to a more continuous shrinkage of coefficients.\n",
    "Feature Selection:\n",
    "\n",
    "Lasso: Inherently performs feature selection, making it suitable when there is a suspicion that many features are irrelevant or redundant.\n",
    "Ridge: Shrinks coefficients towards zero but rarely eliminates them entirely, making it less effective for feature selection.\n",
    "When to Use Lasso:\n",
    "\n",
    "Use Lasso when you suspect that only a subset of features is relevant and you want to perform feature selection.\n",
    "When dealing with high-dimensional data where many predictors may be irrelevant.\n",
    "In summary, Lasso regularization is particularly useful when feature selection is a priority and there is a desire to simplify the model by setting some coefficients to exactly zero. Ridge regularization, on the other hand, tends to shrink coefficients towards zero without driving them exactly to zero and is more suitable when all features are expected to contribute. The choice between Lasso and Ridge often depends on the specific characteristics of the data and the modeling goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71af54-b828-4df2-89c1-3c77abfcc370",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57625d9-98c8-4e81-8c54-fc61c4cbd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the objective function, which discourages overly complex models with large coefficients. Overfitting occurs when a model learns the training data too well, capturing noise and outliers instead of the underlying patterns. Regularization techniques, such as Lasso and Ridge, provide a balance between fitting the training data and maintaining model simplicity.\n",
    "\n",
    "Let's take a look at how Lasso regularization helps prevent overfitting with a simple linear regression example:\n",
    "\n",
    "Example:\n",
    "Consider a dataset with a single independent variable (\n",
    "�\n",
    "X) and a dependent variable (\n",
    "�\n",
    "Y). A regular linear regression model aims to minimize the sum of squared errors:\n",
    "\n",
    "Objective function for linear regression\n",
    "=\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "Objective function for linear regression=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (Y \n",
    "i\n",
    "​\n",
    " − \n",
    "Y\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "\n",
    "Now, let's introduce Lasso regularization. The objective function with Lasso regularization becomes:\n",
    "\n",
    "Objective function with Lasso regularization\n",
    "=\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "^\n",
    "�\n",
    ")\n",
    "2\n",
    "+\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "Objective function with Lasso regularization=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (Y \n",
    "i\n",
    "​\n",
    " − \n",
    "Y\n",
    "^\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "λ is the regularization parameter.\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣ represents the sum of the absolute values of the regression coefficients.\n",
    "The addition of the regularization term \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣ penalizes the model for having large coefficients. As a result:\n",
    "\n",
    "Prevention of Overfitting:\n",
    "\n",
    "The regularization term discourages the inclusion of unnecessary features by driving some coefficients to exactly zero (feature selection).\n",
    "By penalizing large coefficients, the model becomes less sensitive to noise and outliers in the training data, preventing overfitting.\n",
    "Simplicity and Generalization:\n",
    "\n",
    "The regularization term promotes model simplicity by discouraging overly complex models that fit the training data too closely.\n",
    "The model's generalization performance is improved because it is less likely to perform well on the training data but poorly on new, unseen data.\n",
    "Tuning Parameter (\n",
    "�\n",
    "λ):\n",
    "\n",
    "The choice of the regularization parameter (\n",
    "�\n",
    "λ) allows the practitioner to control the trade-off between fitting the training data and maintaining model simplicity. Cross-validation is often used to find an optimal value for \n",
    "�\n",
    "λ.\n",
    "In summary, regularized linear models, such as Lasso, provide a mechanism to control the complexity of the model, preventing overfitting by penalizing large coefficients. The regularization term encourages a balance between fitting the training data well and maintaining a simpler model that generalizes better to new, unseen data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40782d0-3af8-4ecd-bc1e-eec1a42d338f",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a5ce24-1142-462a-9af5-33bdeee62fde",
   "metadata": {},
   "source": [
    "While regularized linear models, such as Lasso and Ridge regression, offer several advantages in preventing overfitting and improving model generalization, they also come with certain limitations that may make them less suitable in specific situations. Here are some limitations of regularized linear models:\n",
    "\n",
    "Loss of Important Features:\n",
    "\n",
    "Regularization can lead to the elimination of certain features by driving their coefficients to zero. If some important features are incorrectly penalized, the model may lose predictive power.\n",
    "Sensitivity to Outliers:\n",
    "\n",
    "Regularized linear models, especially Lasso, can be sensitive to outliers. Outliers may disproportionately influence the regularization process and lead to suboptimal coefficient estimates.\n",
    "Choice of Regularization Parameter:\n",
    "\n",
    "The performance of regularized models is sensitive to the choice of the regularization parameter (\n",
    "�\n",
    "λ). Selecting an appropriate value requires careful tuning, and a poor choice may lead to underfitting or overfitting.\n",
    "Assumption of Linearity:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the features and the target variable. If the true relationship is highly nonlinear, regularized linear models may not capture complex patterns in the data.\n",
    "Not Ideal for Every Dataset:\n",
    "\n",
    "Regularized linear models may not be the best choice for every dataset. In cases where there is no multicollinearity, and the number of features is not excessively large, traditional linear regression may perform well without the need for regularization.\n",
    "Computational Complexity:\n",
    "\n",
    "The optimization problem associated with regularization can be computationally intensive, especially when dealing with large datasets. Training regularized models may take more time compared to non-regularized counterparts.\n",
    "Less Effective for Highly Correlated Features:\n",
    "\n",
    "Regularization is more effective when dealing with datasets where features are somewhat correlated but not highly correlated. In the presence of strong multicollinearity, Ridge regression might be preferred over Lasso.\n",
    "Lack of Interpretability:\n",
    "\n",
    "Regularized models may produce sparse solutions (especially Lasso), making the interpretation of coefficients more challenging, particularly if many coefficients are driven to zero.\n",
    "Domain-Specific Considerations:\n",
    "\n",
    "In some domains or industries, interpretability and understanding the impact of individual features are crucial. Regularized models may sacrifice interpretability for predictive accuracy.\n",
    "Limited Performance Improvement in Some Cases:\n",
    "\n",
    "In scenarios where the number of features is not excessively large, and there is no severe overfitting issue, the application of regularization may not provide substantial performance improvements.\n",
    "In summary, while regularized linear models are powerful tools for addressing overfitting and improving model generalization, their application should be carefully considered in light of the specific characteristics and requirements of the dataset. It's essential to weigh the advantages and disadvantages and, in some cases, explore alternative modeling approaches based on the nature of the data and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ab882-f00f-4342-b92f-0bd994b7e4a4",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b91e6-b43a-459e-8734-87cd45cb51e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd96a4-358c-451e-b660-64af75df2635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ffc28-8263-469e-af57-871ddaba776c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
